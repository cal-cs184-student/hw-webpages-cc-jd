<html>
	<head>
		<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default'></script>
		<link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600&display=swap" rel="stylesheet">
		<style>
			h1 {
				text-align: center;
			}

			.container {
				margin: 0 auto;
				padding: 60px 20%;
			}

			figure {
				text-align: center;
			}

			img {
				display: inline-block;
			}

			body {
				font-family: 'Inter', sans-serif;
			}
		</style>
	</head>
	<body>
		<div class="container">
		<h1>CS184/284A Spring 2025 Homework 3 Write-Up</h1>
		<div style="text-align: center;">Names: Julia Dashzeveg & Cynthia Chen </div>

		<br>

		Link to webpage: <a href="https://cal-cs184-student.github.io/hw-webpages-cc-jd/hw3/index.html">cal-cs184-student.github.io/hw-webpages-cc-jd/hw3/index.html</a>
		Link to GitHub repository: <a href="https://github.com/cal-cs184-student/hw-webpages-cc-jd/edit/master/hw3/index.html">github.com/cal-cs184-student/hw-webpages-cc-jd/edit/master/hw3/index.html</a>
		
		<figure>
			<img src="cornell.png" alt="Cornell Boxes with Bunnies" style="width:70%"/>
<!-- 			<figcaption>You can add images with captions!</figcaption> -->
		</figure>

		<!--
		We've already added one heading per part, to make your write-up as navigable when grading. Please fit your write-up within these sections!
		-->

		<h2>Overview</h2>
		In this homework, we implemented a ray tracing renderer that simulates global illumination, direct lighting, and camera ray generation. Its main functions involve tracing rays through a scene, determining intersections with geometric primitives like spheres and triangles, and calculating the resulting lighting at each intersection point. We also integrated several advanced features like adaptive sampling, which optimizes the number of samples taken based on variance, and used a bounding volume hierarchy to accelerate ray-primitive intersection tests to improve performance. We also implemented a technique for handling multiple bounces of light to simulate indirect illumination and global illumination effects.
Through this homework, we learned how to effectively manage recursive ray tracing for realistic lighting effects and how performance optimizations, like BVH and adaptive sampling, can significantly improve the efficiency of the rendering process. Debugging and fine-tuning the balance between quality and performance also provided valuable insights into the inner workings of physically-based rendering techniques.

		<h2>Part 1: Ray Generation and Scene Intersection</h2>
		In part 1, we implemented the ray generation and primitive intersection parts of the rendering pipeline. It begins with generating primary rays from the camera, which is handled by the generate_ray method to convert pixel coordinates into rays in world space based on the camera’s parameters. These rays originate from the camera position and shoot out into the scene in directions determined by the camera's projection. The raytrace_pixel method takes these camera rays and determines their radiance contribution by tracing them through the scene. It does this by sampling multiple rays per pixel, estimating radiance using the global illumination method, and averaging the results to determine the final color for that pixel.
                <p>Once a ray is traced, we need to check if it intersects with any objects in the scene. Thus, we implemented the has_intersection and intersect methods for primitives like triangles and spheres. For ray-triangle intersection, we can use the Möller-Trumbore algorithm to check if there’s a hit, and for ray-sphere intersection, we can solve the quadratic equation. These functions determine whether the ray hits an object, where it hits, and populates the input Intersection structure with information that will be used later for shading calculations.
                </p>
		<p>Ray-Triangle Intersection Algorithm: the Möller-Trumbore algorithm is used to efficiently determine whether a ray intersects a given triangle in 3D space. We start by computing two edge vectors from the triangle’s three vertices, which help define its plane. Next, we calculate the determinant using the cross product of the ray’s direction and one of the edges, followed by a dot product with the other edge. If this determinant is near zero, the ray is parallel to the triangle, so there’s no intersection. 
To determine the exact intersection point, we compute the barycentric coordinates u and v, which indicate whether the point lies inside the triangle, by calculating dot products with vectors derived from the ray’s origin and triangle edges. If u or v fall outside their valid range, or if their sum is more than 1, the intersection lies outside the triangle. If the intersection is valid, we compute the intersection distance t along the ray and ensure it falls within the ray’s bounds. If so, we update the ray’s maximum intersection distance so that the closest intersection is recorded. Finally, the surface normal is computed using barycentric interpolation of the triangle’s vertex normals to ensure smooth shading, and the intersection data is stored. 
</p>

<!-- 		<p>Here is an example 2x2 gridlike structure using an HTML table. Each <b>tr</b> is a row and each <b>td</b> is a column in that row. You might find this useful for framing and showing your result images in an organized fashion.</p> -->
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
				  <img src="part1-1.png" width="400px"/>
				  <figcaption>empty room</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="part1-2.png" width="400px"/>
				  <figcaption>gems</figcaption>
				</td>
		                <td style="text-align: center;">
				  <img src="part1-3.png" width="400px"/>
				  <figcaption>coil</figcaption>
				</td>
			  </tr>
			</table>
		</div>
		
		<h2>Part 2: Bounding Volume Hierarchy</h2>
		The BVH construction algorithm recursively partitions a set of primitives into a tree structure to accelerate ray-object intersection tests. First, we compute a bounding box that encloses all primitives. If the number of primitives in the current partition is less than or equal to the maximum leaf size, a leaf node is created to store them. Otherwise, the algorithm finds a suitable splitting axis and point to divide the primitives into two subgroups.
The splitting heuristic we used selects the axis with the largest extent from the bounding box of primitive centroids, to ensure a more balanced partition. If the y-axis has the largest extent, we split along it; otherwise, we compare the x-axis and z-axis, selecting the one with the greater extent. The actual split point is chosen as the centroid of this bounding box along the chosen axis. We then partitioned primitives based on whether their centroids fall to the left or right of this split point. If the partitioning fails (all primitives fall into one group), we go back to splitting at the midpoint of the list. Finally, the algorithm recursively constructs left and right child nodes using the divided primitive sets. 
<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
				  <img src="part2-1.png" width="400px"/>
				  <figcaption>Lucy with BVH acceleration</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="part2-2.png" width="400px"/>
				  <figcaption>Max Planck with BVH acceleration</figcaption>
				</td>
		                <td style="text-align: center;">
				  <img src="part2-3.png" width="400px"/>
				  <figcaption>Dragon with BVH acceleration</figcaption>
				</td>
			  </tr>
			</table>
		</div>
		

		<h2>Part 3: Direct Illumination</h2>
		Direct lighting with uniform hemisphere sampling: this function estimates the direct lighting at a surface point using uniform sampling over a hemisphere. The first step is to create a coordinate system at the intersection point, aligning the surface normal with the z-axis. This helps to transform the directions of incoming and outgoing light into a common frame of reference. The transformation matrix o2w is used to convert local coordinates to world space, and its transpose w2o lets us convert world space coordinates back to local space.
Next, the outgoing direction w_out is calculated as the direction from the hit point towards the camera or viewer. The ray's origin is the intersection point hit_p, and its direction is in the opposite direction of the ray's direction transformed into the local space. The direction is then transformed by w2o to map it into the local coordinate system.
The number of samples is the product of the number of lights in the scene and the area light constant, which determines how many samples will be taken for each light source. This is followed by a loop where it samples incoming light directions wi uniformly from a hemisphere. For each sampled direction, a ray is cast from the intersection point in the direction of wi. The ray is then tested for intersections with other primitives in the scene using the BVH structure, which optimizes the intersection tests.
If the ray intersects an object and that object is a light source (where light source's emission is non-zero), the contribution of that light source to the final lighting is computed. The brdf value is evaluated using the outgoing and incoming directions, and the cosine of the angle between the incoming direction and the surface normal is calculated. In this case, the cosine is the z-component of wi, as the sample is taken from the hemisphere.
The radiance contribution is computed by multiplying the bsdf value with the light source emission and the cosine term, then divided by the pdf of the hemisphere sampling. This contribution is summed, then divided by the number of samples to provide the final estimate for the direct lighting at the intersection point.

	        <p>Direct lighting by importance sampling lights: this function estimates direct lighting at an intersection point using importance sampling, focusing only on light sources rather than sampling uniformly across the hemisphere. First, it sets up a coordinate system aligned with the surface normal and computes the outgoing direction w_out, which points towards the viewer. Then, it iterates through all lights in the scene. If a light is a delta light (point light source), only one sample is taken. Otherwise, multiple samples are taken for area lights to improve accuracy.
For each sample, the function calls sample_L to obtain an incoming light direction (wi), the distance to the light, and the pdf value. If the pdf is non-positive, the sample is skipped. Next, a shadow ray is cast from the intersection point to check if any object blocks the light. The ray starts slightly above the surface (EPS_F) and extends up to just before the light source. If the ray hits another object, we ignore the sample because the point is in shadow.
If the light is visible, we compute the surface response using the bsdf function, which determines how much of the incoming light contributes to the final color. The lighting contribution for this sample is calculated using a Monte Carlo estimator, factoring in the bsdf, the incoming light radiance, and a cosine term for proper shading. If multiple samples were taken, then we average the accumulated contribution. Finally, it returns the estimated direct illumination at the intersection point, accounting for visible light sources and shadows.
</p>
			<p>Analysis: When comparing the bunny images rendered using the two sampling methods for direct lighting, you can see that the image generated using the uniform hemisphere sampling method shows noticeable graininess, with a slightly darker overall hue across the whole image. This graininess suggests that the lighting has not been sampled as efficiently, leading to a more noisy and less refined appearance. In contrast, the image produced with light sampling is smoother and more polished, with each color more uniformly distributed and without any dark or black pixels. The lighting looks more consistent, providing a more accurate and visually pleasing representation, as light sampling effectively focuses on sampling from the light sources themselves and thus reducing noise and improving the quality of the image. 
</p>
			
		<h2>Part 4: Global Illumination</h2>
		To implement our at_least_one_bounce_radiance indirect lighting function that simulates global illumination, we recursively trace rays after the first bounce, and account for indirect lighting by sampling the BSDF and then using Russian roulette to terminate rays with a probability of 0.3. 
Specifically, first we calculate the position of the intersection by taking the ray’s origin and adding the direction scaled by the distance to the intersection. Then, we calculated w_out by taking the negative ray direction in world coordinates. Next, we used our one_bounce_radiance function to get the light from bounce 1, which we will add to L_out, the total radiance. For our recursive base case, we terminate the function and return the total radiance if we reach our predetermined max bounces, or if no more light gets scattered. Then, we sample a new ray direction based on the material properties at the intersection. Then, we create a new ray at that intersection point and send it in the sampled direction. In task three, we implemented Russian roulette to decide whether or not the ray continues, using the coin_flip function with probability 0.3. The indirect light then gets scaled and added to total radiance, which is returned at the end of the function. Additionally, importance sampling prioritizes the more important light paths.

		<h2>Part 5: Adaptive Sampling</h2>
		The function raytrace_pixel implements adaptive sampling to optimize the ray tracing process. This adaptive sampling method allows the path tracer to efficiently allocate resources, focusing computation on areas of the image where further refinement is needed and stopping early in areas where the radiance estimate has already stabilized. The function first initializes variables to track the number of samples, the accumulated radiance, and two variables s1 and s2 to store running statistics needed for variance calculation. For each pixel, the function loops over a specified number of samples, generating random samples within the pixel and converting them to normalized image coordinates. Then a ray is generated from the camera, and the radiance along the ray is estimated using the est_radiance_global_illumination function. The radiance is accumulated into the total radiance for the pixel. Also, the illuminance of the radiance sample is used to update the running sum and squared sum of illuminance values.
After each batch of samples, the function calculates the mean and variance of the radiance estimates. The mean is the average of the illuminance values, while the variance is calculated from the running sums. The uncertainty, I, is then calculated using the formula involving the variance and the number of samples, with a 1.96 multiplier for a 95% confidence interval. If the uncertainty becomes small enough, as defined by the condition I <= mean * maxTolerance, the function breaks out of the sampling loop early. This stops further sampling when the variance is low enough, indicating that additional samples would not significantly improve the estimate of the pixel's radiance. Once all necessary samples are collected, the final radiance is averaged over the total number of samples, and the result is stored in the sample buffer for the pixel. The function also tracks the number of samples used for the pixel in the sampleCountBuffer. 

<!-- 		<h2>(Optional) Part 6: Extra Credit Opportunities</h2>
		Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.
		
		<h2>Additional Notes (please remove)</h2>
		<ul>
			<li>You can also add code if you'd like as so: <code>code code code</code></li>
			<li>If you'd like to add math equations, 
				<ul>
					<li>You can write inline equations like so: \( a^2 + b^2 = c^2 \)</li>
					<li>You can write display equations like so: \[ a^2 + b^2 = c^2 \]</li>
				</ul>
			</li>
		</ul> -->
		</div>
	</body>
</html>
